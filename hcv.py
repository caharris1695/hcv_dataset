# -*- coding: utf-8 -*-
"""HCV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13mzMDMh-a8eNtx_m7q0cB__sg0Vzk0vs
"""

import torch
import torch.nn as nn
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import torch.nn.functional as F
import torch.optim as optim
# Import Files
from google.colab import files
uploaded = files.upload()

"""All attributes except Category and Sex are numerical. The laboratory data are the attributes 5-14.

1) X (Patient ID/No.) 

2) Category (diagnosis) (values: '0=Blood Donor', '0s=suspect Blood Donor', '1=Hepatitis', '2=Fibrosis', '3=Cirrhosis')

3) Age (in years)

4) Sex (f,m)

5) ALB

6) ALP

7) ALT

8) AST

9) BIL

10) CHE

11) CHOL

12) CREA

13) GGT

14) PROT
"""



# Read in the dataset with pandas 
df = pd.read_csv("hcvdat0.csv")

# Head of the Data Frame
df.head()

# Basic Describe and Info of Dataframe 

print("Describe")
print(df.describe())

print("Info")
print(df.info())

# See which column of the Data has null values
nulls = [(i, df[i].isnull().sum()) for i in df.columns ]

for column, num_nulls in nulls:
  print(f"Column: {column}, Number of Missing values {num_nulls}")

# Get rid of Rows with missing values 
df = df.dropna(axis=0)

# See if the Data has categorical values
uniques = [(i, np.unique(df[i])) for i in df.columns ]

for column, values in uniques:
  print(f"Column: {column}, Number of unique values {len(values)}")

# Look at Count of each class given sex 
count_plot = sns.countplot(x='Category',hue='Sex',data=df)
count_plot.set_xticklabels(labels=count_plot.get_xticklabels(),rotation=45)

sns.distplot(df['Age'])

sns.distplot(df['ALB'])

sns.distplot(df['CREA'])

# Make the Sex, Category columns Index values instead of text values  
df_sex_classes = df.Sex.unique()
class2idx = {cls:i for cls,i in zip(df_sex_classes, range(len(df_sex_classes)))}
df['Sex'].replace(class2idx, inplace=True)

df_category_classes = df.Category.unique()
class2idx_category = {cls:i for cls,i in zip(df_category_classes, range(len(df_category_classes)))}
df["Category"].replace(class2idx_category, inplace=True)

# Standardize the Numerical Columns 

numerical_columns = []
for col in df.columns:
  if col != "Category" and col != "Sex":
    numerical_columns.append(col)

from sklearn.preprocessing import StandardScaler 

standardizer = StandardScaler()

df[numerical_columns] = standardizer.fit_transform(df[numerical_columns])
print(df.head())

# Split the data into training and testing 
from sklearn.model_selection import train_test_split 

X_train, X_test, y_train, y_test = train_test_split(df[numerical_columns+['Sex']].values, df['Category'].values, test_size=0.2)

# Convert the data to PyTorch Format
Xtrain = torch.tensor(X_train, dtype=torch.float)
Xtest = torch.tensor(X_test, dtype=torch.float)
ytrain = torch.LongTensor(y_train)
ytest = torch.LongTensor(y_test)

# Define the model, output, loss, and optimizer

class Model(nn.Module):

  def __init__(self,in_features=1, hidden_1=5,hidden_2=5,out_features=1):
    # Basically just a standard FF nueral network
    super().__init__()
    self.hidden_1 = nn.Linear(in_features,hidden_1)
    self.hidden_2 = nn.Linear(hidden_1,hidden_2)
    self.out = nn.Linear(hidden_2,out_features)

  def forward(self, x):
    x = F.relu(self.hidden_1(x))
    x = F.relu(self.hidden_2(x))   
    # x = F.relu(self.h3(x))     
    x = self.out(x) 
    return x

# Number of Out Features = Targets 
out_feature_count = len(np.unique(y_train))
# In Features = Number of Columns 
in_feature_count = len(X_train[0,:])

# Create the Model from the Model Class
model = Model(in_features=in_feature_count,out_features=out_feature_count)

# Cross Entropy Loss for Classification 
criterion = nn.CrossEntropyLoss()

# Hyperparameters
EPOCHS = 2500
learning_rate = .08
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Gradient Descent

losses = []

for i in range(EPOCHS):
    i+=1
    y_pred = model.forward(Xtrain)
    loss = criterion(y_pred, ytrain)
    losses.append(loss)
    
    if i%100 == 1:
        print(f'epoch: {i:2}  loss: {loss.item():10.8f}')

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Making predictions

correct = 0 

with torch.no_grad():
    
    for i, data in enumerate(Xtest):
        
        y_val = model.forward(data)
        
        print(f'Data {i+1}.) {str(torch.argmax(y_val))} Real Value - {ytest[i]}')
        
        
        if y_val.argmax().item() == ytest[i]:
            correct+=1
print(correct/len(ytest))

